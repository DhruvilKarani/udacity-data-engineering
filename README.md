## Song Dataset

The first dataset in `data/` is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## Log Dataset
The second dataset in `data/` consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

# Data Modeling with Postgres

The project intends to model data for a music streaming app on postgres. The scripts create (or re-create) a database with 5 tables, following a star schema

1. songplay_table - this is the fact table that contains logs of  music streams. (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
2. user_table - a dimenstion table with user information. (user_id, first_name, last_name, gender, level)
3. song_table - a dimenstion table with song information. (song_id, title, artist_id, year, duration)
4. artist_table - a dimenstion table with artist information. (artist_id, name, location, latitude, longitude)
5. time_table - a dimenstion table with time information. (start_time, hour, day, week, month, year, weekday)


## To create tables
`python create_tables.py`

## To run ETL pipelines on the created tables
`python etl.py`


## To run sanity checks
run test.ipynb

